---
title: "Untitled"
author: "Lorenzo"
date: "2023-04-07"
output: pdf_document
---

# Abstract


L'obiettivo di questo progetto è approfondire la procedura di *bagging*. Al fine di approfondire il funzionamento di questa tecnica è stata effettuata un'implementazione in R, sia per l'obiettivo di classificazione che per quello di regressione. Per implementare il *bagging* è stato utilizzato il pacchetto *caret*. Il lavoro è stato così strutturato: una preliminare fase di analisi esplorativa del dataset, una seconda fase di selezione delle features sulla base di alcune caratteristiche di esse e successivamente testing e valutazione dei modelli. I modelli scelti per l'analisi sono gli alberi di decisione come weak learner e le stime ottenute da essi sono state poi confrontate con quelle ottenute dalla procedura di *bagging* al fine di valutarne i benefici.  


# Introduzione


Il *bagging*, o bootstrap aggregation, è una procedura utilizzata per ridurre la varianza di un metodo di apprendimento statistico. Tale tecnica si basa sul concetto di bootstrap, una procedura di ricampionamento che crea b nuovi bootstrap samples, campioni con reinserinento provenienti dal training set, e attraverso questi è possibile valutare l'accuratezza della stima di un parametro o di una previsione. Il *bagging* sfrutta proprio tale procedura per migliorare le stime o la stessa previsione. 


## Bootstrap


...


## Bagging

Come detto in precedenza, il *bagging* è designato per migliorare la stabilità e accuratezza degli algoritmi di regressione e classificazione. Mediante la media il *bagging* riesce a ridurre la varianza e minimizzare l'overfitting. I bootstrap aggregating (bagging) prediction models sono quindi metodi generali per addestrare molteplici versioni di un modello di previsione e poi combinare (ensembling) queste in un unica previsione aggregata. 

Bagging è un algoritmo piuttosto semplice nel quale b bootstrap samples vengono creati dal training data, l'algoritmo di classificazione o regressione (commonly referred to as the base learner) viene applicato ad ogni bootstrap sample e, nel caso della regressione, le nuove previsioni sono fatte mediando le previsioni dei vari base learners. Quando trattiamo invece il caso di classificazione, le previsioni dei base learner sono combinated usando la classe più votata o mediando le probabilità delle varie classi stimate insieme. Per via del processo di aggregazione, il bagging riduce la varianza di un base learner; tuttavia, il bagging non sempre a migliorare le prestazioni del base learner. Bagging funziona specialmente bene per instabili base learners con alta varianza, nei quali gli output riscontrano grandi cambianti in risposta a piccoli cambiamenti nel training dataset. Fra questi algoritmi sono sicuramente da evidenziare ad esempio gli alberi decisionali. Per quanto riguarda invece algoritmi più stabili o con un elevato bias, il bagging offre pochi miglioramenti sulle previsioni, dal momento che c'è poca variabilità.

Come detto il bagging viene quindi solitamente applicato agli alberi decisionali. Usare tanti alberi non porta all'overfitting, ma è importante realizzare che, dal momento che molteplici modelli vengono fatti girare, per numerose iterazioni il costo computazionale e il tempo richiesto sarà elevato.
Un ulteriore beneficio del *bagging*, che si basa sul ricampionamento con reinserimento, è che può fornire la sua stima interna della performance previsiva del modelllo con *out-of-bag* (OOB) sample. *OOB* sample può essere usato per testare la performance previsiva e i risulati solitamente rispetto al k-fold CV performano bene, assumendo che il dataset sia sufficientemente grande (n ≥ 1000). Di conseguenza, come il dataset diventa più grande e le iterazioni del bagging crescono, è comune usare l' *OOB* error estimate come proxy per la performance previsiva del modello.

The process of bagging involves fitting models to each of the bootstrap samples which are completely independent of one another. This means that each model can be trained in parallel and the results aggregated in the end for the final model. Consequently, if you have access to a large cluster or number of cores, you can more quickly create bagged ensembles on larger data sets.


## Boosting e differenze con il bagging



# Prerequisiti

Faremo uso dei seguenti pacchetti per effettuare l'analisi sul dataset scelto.

```{r setup}
library(dplyr) 
library(ggplot2)
library(plotly)
library(gmodels)
library(rpart)      
library(caret) 
library(rpart.plot)  
library(vip)         
library(pdp) 
library(tibble)
library(forcats)
library(doParallel) 
library(foreach)

data<- read.csv("/Users/lorenzobruni/Desktop/WA_Fn-UseC_-Telco-Customer-Churn.csv")
data<-na.omit(data)
```

#Analisi esplorativa

In questa sezione, al fine di conoscere meglio il dataset con cui è stato condotto lo studio, è stata effettuata un'analisi esplorativa per indagare meglio la struttura delle variabili su cui basare il modello e per supportare le decisioni intraprese nella parte di feature selection.

```{r, echo=FALSE}
dim(data)
str(data)
```
Come possiamo vedere il dataset di churn scelto è composto da **7032** righe e **21** colonne. Fra queste abbiamo: *CustomerID*, *gender*, *SeniorCitizen* (variabile dicotomica che indica se il cliente è un cittadino anziano o no), *Partner*, *Dependents* (se il cliente ha persone a carico o meno), *tenure* (numero di mesi che il consumatore è rimasto con l'azienda), una serie di variabili legate hai servizi del cliente (*PhoneService*, *MultipleLines*, *InternetService*, *OnlineSecurity*, *OnlineBackup*, *DeviceProtection*, *TechSupport*, *StreamingTV*, *StreamingMovies*),  *Contract* (variabile che indici i termini del contratto del cliente), *PaperlessBilling* (se il cliente ha o meno una fatturazione senza carta), *PaymentMethod*, *MonthlyCharges* (l'importo addebitato mensilmente al cliente), *TotalCharges* (l'importo totale addebitato al cliente), *Churn.*

```{r}
summary(data$tenure)
sd(data$tenure)
```

Particolarmente interessante per la nostra analisi di regressione è lo studio della variabile target *tenure*. Come possiamo vedere la deviazione standard di questa variabile è molto elevato pari a **24.5**, con una media di **32.4**.

```{r, echo=FALSE}
ggplot(data = data, aes(x = "", y = tenure)) + 
  geom_boxplot(width = 0.4, fill = "white") +
  labs(title = 'Tenure distribution',
       y='Tenure',x='')+
  coord_cartesian()
```

Possiamo vedere l'elevata variabilità di questa variabile anche dallo studio del relativo boxplot presente nella precedente figura.

```{r, echo=FALSE}
df <- data %>%
  group_by(Churn) %>%
  summarise(counts = n())

ggplot(df, aes(x = Churn, y = counts)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  labs(title = 'Churn distribution')
```

Inoltre, per quanto riguarda la variabile di *churn* vediamo dal precedente grafico come il numero di clienti che hanno effettuato il churn sia nettamente inferiore rispetto a coloro che non lo hanno fatto.

```{r, echo=FALSE}
ggplot(data = data, aes(x=Churn,y=tenure, fill=Churn)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  labs(title = 'Churn vs tenure',
       y='tenure',x='Churn')
```

Dal precedente grafico è possibile vedere che la variabile *tenure* è una variabile potenzialmente utile nelllo spiegare il comportamento di *churn* dei clienti, indicando che clienti con un numero di mesi trascorsi con l'azienda maggiore presenta una minore probabilità di abbandono.


```{r, echo=FALSE}
ggplot(data = data, aes(x=Churn,y=TotalCharges, fill=Churn)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  labs(title = 'Churn vs TotalCharges',
       y='TotalCharges',x='Churn')
```

Come possiamo vedere dal grafico sovrastante la variabile *TotalCharges* rappresenta anch'essa un fattore potenzialmente determinante nello spiegare il comportamento della variabile di *Churn*. Maggiore l'ammontare totale speso, minore la probabilità di abbandono.

```{r, echo=FALSE}
ggplot(data = data, aes(x=Churn,y=MonthlyCharges, fill=Churn)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  labs(title = 'Churn vs MonthlyCharges',
       y='MonthlyCharges',x='Churn')
```

Come possiamo vedere dal grafico sovrastante la variabile *TotalCharges* rappresenta anch'essa un fattore potenzialmente determinante nello spiegare il comportamento della variabile di *Churn*. Maggiore l'ammontare totale speso, minore la probabilità di abbandono.

Data la dipendenza tra le variabili *TotalCharges* e *MonthlyCharges*, decidiamo di escludere una delle due variabili e teniamo per l'analisi la variabile *MonthlyCharges*. 

Ragionamento simile per le variabili *Phoneservice* e *MultipleLines* nel quale la seconda è inevitabilmente legata ai valori assunti dalla prima. Decidiamo così di includere nell'analisi solo la variabile *Phoneservice*.

Lo stesso ragionamento è stato effettuato per quanto riguarda il legame tra la variabile *InternetService* e le variabili *OnlineSecurity*, *OnlineBackup*, *DeviceProtection*, *MonthlyCharges*, *TechSupport*, *StreamingTV*, *StreamingMovies*. Anche in questo caso decidiamo di tenere in considerazione esclusivamente la variabile InternetService poichè i valori assunti dalle altre variabili risultano influenzati dai valori di quest'ultima.


# Analisi di Regressione

L'analisi di regressione viene effetuata cercando di prevedere i valori assunti dalla variabile tenure mediante l'utilizzo delle altre variabili rimaste come regressori. 


```{r, echo=FALSE }
data <- subset(data,select=-c(customerID, Churn, MultipleLines, OnlineSecurity, OnlineBackup, DeviceProtection, TotalCharges, TechSupport, StreamingTV, StreamingMovies))
set.seed(1234)
split <- sample(nrow(data), floor(0.7*nrow(data)))
traindf <- data[split,]
testdf <-  data[-split,]
```


## Albero di regressione

Come detto in precedenza il modello semplice che decidiamo di utilizzare come weak-learner è l'albero decisionale. Per la costruzione dell'albero di regressione usiamo la funzione *rpart()* contenuta nel pacchetto rpart, dopodiché per la visualizzazione dell'albero utilizziamo la funzione *rpart.plot()*. Il processo di addestramento e visualizzazione del modello è molto simile sia per la parte di regressione che per quella di classificazione.

```{r}
fit <- rpart(
  formula = tenure ~ .,
  data    = traindf,
  method  = "anova"
)
```


```{r, echo=FALSE}
rpart.plot(fit)
```

La variabile che decreta il primo split (la variaile che retituisce la più grande riduzione nel SSE) è *Contract*.

Visualizzando il modello ad albero con *rpart.plot()*, una cosa che si può notare è che l'albero contiene 6 nodi interni e 7 risultanti nodi terminali. 

Di default *rpart()* automaticamente applica un range di valori di cost complexity (*cp* values) per potare l'albero. Per confrontare l'errore associato a ciascun *cp* value, *rpart()* performa un 10-fold CV.

```{r, echo=FALSE}
plotcp(fit)
```

Il grafico di pruning complexity parameter (*cp*) illustra il relativo cross validation error (y-axis) per vari cp values (lower x-axis). Piccoli valori di *cp* portano ad alberi più grandi (upper x-axis). 
Dal precedente grafico si può notare come un vlore ottimale di *cp* è **0.031**, offrendo un buon bilanciamento tra complessità del modello e relativo errore.

```{r}
pruned <- prune(fit, cp=0.031)
preds <- predict(pruned, testdf)
rmse <- RMSE(
   pred = preds,
   obs = testdf$tenure
)
rmse
```

Il root mean square error ottenuto valutando il modello finale sul test set è pari a **17.05**. Considerando che lo scarto quadratico medio per la variabile *tenure* come visto in precedenza è **24.5**, il risultato ottenuto dal nostro modello può essere considerato soddisfacente.

Per misurare l'importanza che le varie feature assumono nello spiegare il comportamento della variabile target, viene considerata la riduzione nella funzione di perdita (ovvero, SSE) attribuita ad ogni variabile ad ogni split. In alcuni casi, una variable potrebbe essere usata molte volte in un albero; di conseguenza, la riduzione totale nella funzione di perdita causata da una variabile nei vari split sono sommate e usate per la feature importance. Gli alberi decisionali eseguono automaticamente feature selection dal momento che le feature non informative non vengono usate dal modello.

```{r, echo=FALSE}
fit$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Regression")
```

Dal precedente grafico, che illustra l'importanza che le varie features hanno nello spiegare il comportamento della variabile target, sono sicuramente da evidenziare le variabili *Contract*, *PaymentMethod* e *MonthlyCharges*.

In conclusione, gli alberi decisionali hanno diversi vantaggi: 
- richiedono poco pre-processing, questo non vuol dire che il feature engineering non migliori le prestazioni del modello, ma piuttosto che non ci sono particolari requisiti di pre-processing;
- Solitamente gli outliers non distorgono tanto i risultati, dal momento che il partizionamento binario semplicemente guarda alla singola istance per decretare lo split all'interno una distribuzione di feature. 
- Gli alberi decisionali possono facilmente gestire categorical features senza pre-processing. Tuttavia, essi spesso non raggiungono ottimi risultati in termini di performance.


## Bagging

I Bootstrap aggregating (*bagging*) prediction models sono un metodo generale per addestrare molteplici versioni di un modello di previsione e poi combinando (ensembling) i vari risultati in una previsione aggregata. 

La performance ottimale è spesso trovata unendo 50–500 alberi. Dataset con pochi previsori richiedono spesso meno alberi; mentre i set di dati con molto rumore o più predittori forti potrebbero aver bisogno di più alberi.  

Per questa analisi decidiamo di utilizzare 100 alberi non potati (non potando gli alberi stiamo mantenendo una bassa distorsione ma un elevata varianza ed è in questo caso che è possibile ottenere un effetto maggiore dal bagging). Una cosa da notare è che tipicamente, più alberi vengono utilizzati e migliore saranno la prestazioni del modello di bagging, dal momento che aggiungendo più alberi mediamo tra più modelli decisionali ad elevata varianza. Tipicamente, l'errore di previsione si appiattisce e si stabilizza una volta raggiunto un numero adeguato di alberi. Spesso, sono necessari circa 50–100 alberi per stabilizzare l'errore (in altri casi sono necessari 500 o più).

```{r}
set.seed(1234)
bag_model <- train(
   tenure ~ ., 
   data = traindf, 
   nbagg = 100,
   trControl = trainControl(method = "cv", number = 10),
   method = "treebag",
   control = rpart.control(minsplit = 2, cp = 0)
   )
bag_model
```


```{r, echo=FALSE}
cs_preds_bag <- bind_cols(
   Predicted = predict(bag_model, newdata = testdf),
   Actual = testdf$tenure
)
(cs_rmse_bag <- RMSE(pred = cs_preds_bag$Predicted, obs = cs_preds_bag$Actual))
```
Il mean squared error ottenuto dal nostro modello risulta pari a  .



Sfortunatamente, per via del processo di bagging, modelli come gli alberi decisionali che sono percepiti come interpretabili e visualizzabil adesso non lo sono più. Tuttavia, possiamo ancora fare inferenza su come le varie features influenzano il nostro modello. Per i bagged decision trees, questo processo è simile a quello degli alberi decisionali. Per ciascun albero, si calcola la somma delle funziona di perdita fra tutti gli split. Dopodiché per ciascuna features si aggrega questa misura per tutti gli alberi. Le features con la più grande riduzione nel SSE (per la regressione) sono considerate importanti. 

```{r, echo=FALSE}
plot(varImp(bag_model), main="Variable Importance with Bagging")
```

```{r, echo=FALSE}
cs_scoreboard <- rbind(data.frame(Model = "Single Tree", RMSE = rmse),
   data.frame(Model = "Bagging", RMSE = cs_rmse_bag)
) %>% arrange(RMSE)
cs_scoreboard
```

Confrontando il root mean squared error del modello di bagging con quello ottenuto dal decision tree, vediamo come siamo riusciti ad ottenere una buona riduzione dell'errore commesso dal precedente modello.

# Conclusioni

Il processo di *bagging* migliora l'accuratezza delle previsioni per modelli ad elevata varianza (e basso bias) a spese dell'interpretabilità e della velocità computazionale. Tuttavia, usando vari algoritmi e strumenti di interpretabilità, possiamo ancora fare inferenza su come il nostro bagged model sfrutta la feature information. Inoltre, dal momento che il bagging consiste in una serie di processi indipendenti, l'algoritmo risulta facilmente parallelizzabile.
Tuttavia, con il processo di bagging degli alberi un problema continua a sussistere. Nonostante il modello esegue i vari step in maniera indipendente, gli alberi nel processo di bagging non sono completamente indipendenti tra di loro, dal momento che tutte le features sono considerate ad ogni split di ogni albero. Di conseguenza, alberi provenienti da diversi bootstrap samples hanno una struttura simile fra loro (specialmente nella parte iniziale dell'albero) a causa di eventuali relazioni forti sottostanti. Questa caratteristica è conosciuta come **tree correlation** e previene il *bagging* da ridurre ulteriormente la varianza del base learner. Gli algoritmi di Random forest estendono e migliorano i bagged decision trees riducendo questa correlazione e migliorando così la precisione dell'insieme complessivo.

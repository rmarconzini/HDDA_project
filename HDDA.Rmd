---
title: "Untitled"
author: "Lorenzo"
date: "2023-04-07"
output: pdf_document
---

# Abstract


L'obiettivo di questo progetto è approfondire la procedura di bagging. Al fine di approfondire il funzionamento di questa tecnica è stata effettuata un'implementazione in R, sia per l'obiettivo di classificazione che per quello di regressione. Per implementare il bagging è stato utilizzato il pacchetto caret. Il lavoro è stato così strutturato: una preliminare fase di analisi esplorativa del dataset, una seconda fase di selezione delle features sulla base di alcune caratteristiche di esse e successivamente testing e valutazione dei modelli. Come algoritmi sono stati scelti gli alberi di decisione come weak learner e le stime ottenute da essi sono state poi confrontate con quelle ottenute dal bagging al fine di valutare i benefici di tale tecnica.  


# Introduzione


Il bagging, o bootstrap aggregation, è una procedura utilizzata per ridurre la varianza di un metodo di apprendimento statistico. Tale tecnica si basa sul concetto di bootstrap, una procedura di ricampionamento che crea b nuovi bootstrap samples, campioni con reinserinento provenienti dal training set, e attraverso questi è possibile valutare l'accuratezza della stima di un parametro o di una previsione. Il bagging sfrutta proprio tale procedura per migliorare le stime o la stessa previsione. Il bagging è designato per migliorare appunto la stabilità e accuratezza degli algoritmi di regressione e classificazione. Mediante la media del modello il bagging riesce a ridurre la varianza e minimizzare l'overfitting. I bootstrap aggregating (bagging) prediction models sono metodi generali per addestrare molteplici versioni di un modello di previsione e poi combinare (ensembling) queste in un unica previsione aggregata. 
...
Esso viene solitamente applicato agli alberi decisionali, ma tuttavia può essere utilizzato con ogni altro metodo.


# Prerequisiti

Faremo uso dei seguenti pacchetti per effettuare l'analisi sul dataset.

```{r setup}
library(dplyr) 
library(lubridate)
library(ggplot2)
library(plotly)
library(gmodels)
library(rpart)      
library(caret) 
library(rpart.plot)  
library(vip)         
library(pdp) 
library(tibble)
library(forcats)
library(doParallel) 
library(foreach)
library(scorecard)

data<- read.csv("/Users/lorenzobruni/Desktop/WA_Fn-UseC_-Telco-Customer-Churn.csv")
data<-na.omit(data)
```

#Analisi esplorativa

```{r, echo=FALSE}
dim(data)
str(data)
```

```{r, echo=FALSE}
summary(data)
sd(data$tenure)
```



```{r, echo=FALSE}
ggplot(data = data, aes(x = "", y = tenure)) + 
  geom_boxplot(width = 0.4, fill = "white") +
  labs(title = 'Tenure distribution',
       y='Tenure',x='')+
  coord_cartesian()
```

```{r, echo=FALSE}
df <- data %>%
  group_by(Churn) %>%
  summarise(counts = n())

ggplot(df, aes(x = Churn, y = counts)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  labs(title = 'Churn distribution')
```


```{r, echo=FALSE}
ggplot(data = data, aes(x=Churn,y=tenure, fill=Churn)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  labs(title = 'Churn vs tenure',
       y='tenure',x='Churn')
```

```{r, echo=FALSE}
ggplot(data = data, aes(x=Churn,y=TotalCharges, fill=Churn)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  labs(title = 'Churn vs TotalCharges',
       y='TotalCharges',x='Churn')
```

```{r, echo=FALSE}
ggplot(data = data, aes(x=Churn,y=MonthlyCharges, fill=Churn)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  labs(title = 'Churn vs MonthlyCharges',
       y='MonthlyCharges',x='Churn')
```


```{r, echo=FALSE}
corr<- data[, c("MonthlyCharges", "TotalCharges", "tenure")]
cor(corr, method = "pearson", use = "complete.obs")
```

```{r, echo=FALSE}
ggplot(data, aes(TotalCharges,tenure))+geom_point()+geom_smooth(method=lm)
```

Data la dipendenza tra le variabili TotalCharges e MonthlyCharges, decidiamo di escludere una delle due variabili e teniamo per l'analisi la variabile TotalCharges.

Ragionamento simile per le variabili Phoneservice e MultipleLines nel quale la seconda è inevitabilmente legata ai valori assunti dalla prima. Decidiamo così di includere nell'analisi solo la variabile Phoneservice.

Lo stesso ragionamento è stato effettuato per quanto riguarda il legame tra la variabile InternetService e le variabili OnlineSecurity, OnlineBackup, DeviceProtection, MonthlyCharges, TechSupport, StreamingTV, StreamingMovies. Anche in questo caso decidiamo di tenere in considerazione esclusivamente la variabile InternetService poichè i valori assunti dalle altre variabili risultano influenzati dai valori di quest'ultima.


#Analisi di Regressione

L'analisi di regressione viene effetuata cercando di prevedere i valori assunti dalla variabile tenure rispetto alle altre variabili. 


```{r, echo=FALSE }
data <- subset(data,select=-c(customerID, Churn, MultipleLines, OnlineSecurity, OnlineBackup, DeviceProtection, MonthlyCharges, TechSupport, StreamingTV, StreamingMovies))
set.seed(1234)
split <- sample(nrow(data), floor(0.7*nrow(data)))
traindf <- data[split,]
testdf <-  data[-split,]
```


Come detto in precedenza il modello semplice che decidiamo di utilizzare come weak-learner è l'albero decisionale. Per la costruzione dell'albero di regressione usiamo la funzione rpart() contenuta nel pacchetto rpart, dopodiché per la visualizzazione dell'albero utilizziamo la funzione rpart.plot(). Il processo di addestramento e visualizzazione del modello è molto simile sia per la parte di regressione che per quella di classificazione.

```{r}
fit <- rpart(
  formula = tenure ~ .,
  data    = traindf,
  method  = "anova"
)
```


```{r, echo=FALSE}
rpart.plot(fit)
```

La variabile che decreta il primo split (la prima variaile è quella che retituisce la più grande riduzione nel SSE) è TotalCharges.

Visualizzando il modello ad albero con rpart.plot(), una cosa che si può notare è che l'albero contiene 9 nodi interni e 10 risultanti nodi terminali. 

Di default rpart() automaticamente applica un range di valori di cost complexity ( α values per potare l'albero). Per confrontare l'errore associato a ciascun α value, rpart() performa un 10-fold CV.

```{r, echo=FALSE}
plotcp(fit)
```

Il grafico di pruning complexity parameter (cp) illustra il relativo cross validation error (y-axis) per vari cp values (lower x-axis). Piccoli valori di cp portano ad alberi più grandi (upper x-axis). 

```{r, echo=FALSE}
preds <- predict(fit, testdf)
rmse <- RMSE(
   pred = preds,
   obs = testdf$tenure
)
rmse
```

The pruning process leads to an average prediction error of 7.2 in the test data set. This is not bad considering the standard deviation for that variable as seen above is 24.5.

To measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated. In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance. Decision trees perform automated feature selection where uninformative features are not used in the model.
```{r, echo=FALSE}
fit$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Regression")
```
Variable importance based on the total reduction in cross-validated SSE for the tenure decision tree.

Decision trees have a number of advantages. Trees require very little pre-processing. This is not to say feature engineering may not improve upon a decision tree, but rather, that there are no pre-processing requirements. Outliers typically do not bias the results as much since the binary partitioning simply looks for a single location to make a split within the distribution of each feature. Decision trees can easily handle categorical features without preprocessing. However, individual decision trees generally do not often achieve state-of-the-art predictive accuracy.


Bootstrap aggregating (bagging) prediction models is a general method for fitting multiple versions of a prediction model and then combining (or ensembling) them into an aggregated prediction. Bagging is a fairly straight forward algorithm in which b bootstrap copies of the original training data are created, the regression or classification algorithm (commonly referred to as the base learner) is applied to each bootstrap sample and, in the regression context, new predictions are made by averaging the predictions together from the individual base learners. When dealing with a classification problem, the base learner predictions are combined using plurality vote or by averaging the estimated class probabilities together. Because of the aggregation process, bagging effectively reduces the variance of an individual base learner (i.e., averaging reduces variance); however, bagging does not always improve upon an individual base learner.Bagging works especially well for unstable, high variance base learners—algorithms whose predicted output undergoes major changes in response to small changes in the training data.This includes algorithms such as decision trees and KNN (when k is sufficiently small). However, for algorithms that are more stable or have high bias, bagging offers less improvement on predicted outputs since there is less variability.
Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome. 
A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample. The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data set is sufficiently large (say n ≥ 1000). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance.

Rather than use a single pruned decision tree, we can use, say, 100 bagged unpruned trees (by not pruning the trees we’re keeping bias low and variance high which is when bagging will have the biggest effect). One thing to note is that typically, the more trees the better. As we add more trees we’re averaging over more high variance decision trees. Early on, we see a dramatic reduction in variance (and hence our error) but eventually the error will typically flatline and stabilize signaling that a suitable number of trees has been reached. Often, we need only 50–100 trees to stabilize the error (in other cases we may need 500 or more).
```{r}
set.seed(1234)
bag_model <- train(
   tenure ~ ., 
   data = traindf, 
   nbagg = 100,
   trControl = trainControl(method = "cv", number = 10),
   method = "treebag",
   control = rpart.control(minsplit = 2, cp = 0)
   )
bag_model
```
The process of bagging involves fitting models to each of the bootstrap samples which are completely independent of one another. This means that each model can be trained in parallel and the results aggregated in the end for the final model. Consequently, if you have access to a large cluster or number of cores, you can more quickly create bagged ensembles on larger data sets.

```{r, echo=FALSE}
cs_preds_bag <- bind_cols(
   Predicted = predict(bag_model, newdata = testdf),
   Actual = testdf$tenure
)
(cs_rmse_bag <- RMSE(pred = cs_preds_bag$Predicted, obs = cs_preds_bag$Actual))
```

Unfortunately, due to the bagging process, models that are normally perceived as interpretable are no longer so. However, we can still make inferences about how features are influencing our model. For bagged decision trees, this process is similar to the one of the decision tree. For each tree, we compute the sum of the reduction of the loss function across all splits. We then aggregate this measure across all trees for each feature. The features with the largest average decrease in SSE (for regression) are considered most important. 
```{r, echo=FALSE}
plot(varImp(bag_model), main="Variable Importance with Bagging")
```

```{r, echo=FALSE}
cs_scoreboard <- rbind(data.frame(Model = "Single Tree", RMSE = rmse),
   data.frame(Model = "Bagging", RMSE = cs_rmse_bag)
) %>% arrange(RMSE)
cs_scoreboard
```

# Conclusioni

Bagging improves the prediction accuracy for high variance (and low bias) models at the expense of interpretability and computational speed. However, using various interpretability algorithms, we can still make inferences about how our bagged model leverages feature information. Also, since bagging consists of independent processes, the algorithm is easily parallelizable.
However, when bagging trees, a problem still exists. Although the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships. This characteristic is known as tree correlation and prevents bagging from further reducing the variance of the base learner. Random forests extend and improve upon bagged decision trees by reducing this correlation and thereby improving the accuracy of the overall ensemble.
